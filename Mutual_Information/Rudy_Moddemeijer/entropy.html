<html><head><title>entropy (MATLAB Function Reference)</title>

</head>

<body bgcolor=#ffffff>

<table border=0 width="100%" cellpadding=0 cellspacing=0><tr>
<td valign=baseline bgcolor="#d9e1e1"><b>MATLAB Function Reference</b></td>
</tr>
</table>

<P>
<font class=headdoc size=+3 color="#990000">entropy</font>

<p>Estimate the entropy of a stationary signal with independent samples</p>

<p>
<font class=midsup size=+1 color="#990000"><b>Syntax</b></font>
<br>

<ul><pre>
[estimate,Nbias,sigma,descriptor] = entropy(x)
[estimate,Nbias,sigma,descriptor] = entropy(x,descriptor)
[estimate,Nbias,sigma,descriptor] = entropy(x,descriptor,base)
[estimate,Nbias,sigma,descriptor] = entropy(x,descriptor,base,approach)
</pre></ul>

<p>
<font class=midsup size=+1 color="#990000"><b>Description</b></font>
<br>

<p>
Entropy estimation is a two stage process; first a <a href="histogram.html">
histogram </a> is estimated and thereafter the entropy is calculated. For
the explanation of the usage of the <code>descriptor</code> of the histogram
see <a href="histogram.html"> histogram </a>.
<P>
In case of a disrete stochastic variable <code>i</code> in the integer subrange
<code>lower <= i < upper</code> the descriptor should be selected
as <code>[lower,upper,upper-lower]</code>. The
R(epresentation)-unbiased entropy will be estimated.
<P>
In case of a continuous stochastic variable the descriptor can be left
unspecified. In this case the default descriptor of <a href="histogram.html">
histogram </a> will be used.
<P>
The estimate depends on the value of <code>approach</code> 
<ul>
<LI> <code>'unbiased'</code>: a N(umber)-unbiased estimate (default),
<LI> <code>'biased'</code>: a N(umber)-biased estimate and
<LI> <code>'mmse'</code>: a minimum Mean Square Error estimate, obtained by
balancing bias and variance after N-bias correction.
</ul>
The <code>base</code> of the logarithm determines the unit of
measurement. Default base e (nats) is used, alternative choises are 2 (bit)
and 10 (Hartley).
<P>
As a result the function returns the <code>estimate</code>, the N-bias
(<code>Nbias</code>) of the estimate, the estimated standard error <code>sigma</code> and the used <code>descriptor</code>.
<P>

<font class=midsup size=+1 color="#990000"><b>Example</b></font>

<p>
Estimate the entropy of a discrete stochastic variable with probability 0.25
on an observed 0 and probability 0.75 on an observed 1. The entropy is
0.5623 nat.
<br>
<P>
<code>
>> signal(1:250)=0;<br>
>> signal(251:1000)=1;<br>
>> [estimate,nbias,sigma,descriptor] =entropy(signal,[0,2,2],'u')<br>
   estimate = 0.5628<br>
   nbias = 0<br>
   sigma = 0.0151<br>
   descriptor = 0     2     2<br>
</code>
<P>
Estimate the entropy of a Gaussian distributed continuous variable with zero
mean and unit variance. The entropy is 1.4189 nat.
<p>
<code>
>> signal=normrnd(0,1,1,1000);<br>
>> [estimate,nbias,sigma,descriptor] =entropy(signal,[-3,3,30])<br>
   estimate = 1.3643<br>
   nbias = 0<br>
   sigma = 0.0208<br>
   descriptor = -3     3    30<br>
</code>
<p>

<font class=midsup size=+1 color="#990000"><b>See Also</b></font>

<p>
<a href="entropy2.html"> entropy2 </A><BR>
<a href="histogram.html"> histogram </A>
<P>

<font class=midsup size=+1 color="#990000"><b>Literature</b></font>

<p>
Moddemeijer, R.
<I>On Estimation of Entropy and Mutual Information of Continuous Distributions,</I>
Signal Processing,
1989, 
vol. 16,
nr. 3,
pp. 233-246,
<A  HREF="http://www.cs.rug.nl/~rudy/papers/abstracts/RM8902.html"> abstract </A>,
<A  HREF="http://www.cs.rug.nl/~rudy/papers/bibtex/RM8902.bib"> BibTeX </A>,
<P>
For the principle of Minimum Mean Square Error estimation see:
<P>
Moddemeijer, R.
<I><A href="http://www.cs.rug.nl/~rudy/papers/documents/RM9902.ps.gz"> An efficient algorithm for selecting optimal configurations of
AR-coefficients</A></I>,
Twentieth Symp. on Information Theory in the Benelux,
May 27-28, 1999, Haasrode (B),
pp 189-196,
eds. A. Barb&eacute; et. al.,
<A  href=http://ei1.ei.ele.tue.nl/wic> Werkgemeenschap Informatie- en
Communicatietheorie</A>, Enschede (NL),
and 
IEEE Benelux Chapter on Information Theory,
ISBN: 90-71048-14-4,
<A  HREF="http://www.cs.rug.nl/~rudy/papers/abstracts/RM9902.html"> abstract </A>,
<A  HREF="http://www.cs.rug.nl/~rudy/papers/bibtex/RM9902.bib"> BibTeX </A>,
<P>

<font class=midsup size=+1 color="#990000"><b>Source code</b></font>

<p>
<a href="../source/entropy.m"> entropy.m </A>
<P>

<table border=0 width="100%" cellpadding=0 cellspacing=0><tr>
<td valign=baseline bgcolor="#d9e1e1"><b>MATLAB Function Reference</b></td>
</tr>
</table>

<br>
<br>
<center>
Copyright <a href="http://www.cs.rug.nl/~rudy">R. Moddemeijer</a>

</body>
</html>








